import os
import glob
import pandas as pd
from tqdm import tqdm
from bertopic import BERTopic
from sentence_transformers import SentenceTransformer
from umap import UMAP
from sklearn.feature_extraction.text import CountVectorizer

# labeled í´ë” ì§€ì • (ì½ê¸°/ì €ì¥ ëª¨ë‘)
LABELED_FOLDER = r"C:\Users\user\PycharmProjects\PythonProject3\labeled"

# ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸
stopwords = [
    'ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì˜', 'ë¡œ', 'ìœ¼ë¡œ', 'ì™€', 'ê³¼', 'ë„', 'ë§Œ',
    'ì—ì„œ', 'ë¶€í„°', 'ê¹Œì§€', 'ë¡œë¶€í„°', 'ì—ê²Œ', 'ê»˜', 'í•œí…Œ', 'ë³´ê³ ', 'ë”', 'ê°€ì¥', 'ë§¤ìš°',
    'ë„ˆë¬´', 'ì •ë§', 'ì§„ì§œ', 'ì•„ì£¼', 'ì™„ì „', 'ì „í˜€', 'ë³„ë¡œ', 'ì¡°ê¸ˆ', 'ì¢€', 'ë§ì´', 'ì˜',
    'ëª»', 'ì•ˆ', 'ì•Š', 'ë•Œë¬¸', 'ìœ„í•´', 'í†µí•´', 'ëŒ€í•´', 'ê´€í•´', 'ë”°ë¼', 'ì˜í•´', 'ì—ê²Œ',
    'ê·¸ëŸ°', 'ì´ëŸ°', 'ì €ëŸ°', 'ì–´ë–¤', 'ëª¨ë“ ', 'ê°ê°', 'ì—¬ëŸ¬', 'ë‹¤ë¥¸', 'ê°™ì€', 'ë¹„ìŠ·',
    'ë‹¤ì–‘', 'íŠ¹ë³„', 'ì¼ë°˜', 'ê¸°ë³¸', 'ì£¼ìš”', 'ì¤‘ìš”', 'í•„ìš”', 'ê°€ëŠ¥', 'ë¶ˆê°€ëŠ¥',
    'ê·¸ëƒ¥', 'ê·¸ì €', 'ë‹¨ì§€', 'ì˜¤ì§', 'ë‹¤ë§Œ', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ¬ë‚˜', 'ê·¸ëŸ°ë°', 'ë”°ë¼ì„œ',
    'ê·¸ë¦¬ê³ ', 'ë˜í•œ', 'ë˜', 'ì—­ì‹œ', 'ë¬¼ë¡ ', 'ë‹¹ì—°', 'í™•ì‹¤', 'ë¶„ëª…', 'ì•„ë§ˆ', 'í˜¹ì‹œ'
]

umap_model = UMAP(
    n_neighbors=15,
    n_components=5,
    metric='cosine',
    random_state=42
)
embedding_model = SentenceTransformer("BM-K/KoSimCSE-roberta-multitask", device="cuda")


def topic_modeling_per_category_in_file(file_path):
    print(f"\nğŸ“‚ íŒŒì¼ ì²˜ë¦¬ ì‹œì‘: {file_path}")

    df = pd.read_excel(file_path)
    if "category" not in df.columns or "content" not in df.columns:
        print(f"âš ï¸ 'category' ë˜ëŠ” 'content' ì»¬ëŸ¼ì´ ì—†ìŒ â†’ ìŠ¤í‚µ: {file_path}")
        return

    categories = df["category"].unique()

    for cat in categories:
        cat_df = df[df["category"] == cat]
        texts = cat_df["content"].dropna().tolist()

        split_texts = []
        for text in texts:
            paragraphs = [p.strip() for p in text.split("\n") if len(p.strip()) > 30]
            split_texts.extend(paragraphs)

        if len(split_texts) < 10:
            print(f"âš ï¸ {cat}: ë°ì´í„°ê°€ ë„ˆë¬´ ì ì–´ ìŠ¤í‚µ ({len(split_texts)}ê°œ)")
            continue

        print(f"\nğŸš€ [{cat}] í† í”½ëª¨ë¸ë§ ì‹œì‘ (ë¬¸ë‹¨ ìˆ˜: {len(split_texts)})")

        if len(split_texts) < 500:
            min_df_value = 1
        elif len(split_texts) < 2000:
            min_df_value = 2
        else:
            min_df_value = 3

        vectorizer_model = CountVectorizer(
            stop_words=stopwords,
            ngram_range=(1, 2),
            min_df=min_df_value
        )

        topic_model = BERTopic(
            embedding_model=embedding_model,
            umap_model=umap_model,
            vectorizer_model=vectorizer_model,
            top_n_words=10,
            calculate_probabilities=False,
            verbose=False
        )

        try:
            topics, probs = topic_model.fit_transform(split_texts)
            topic_model.update_topics(split_texts, topics, vectorizer_model=vectorizer_model)

            topic_info = topic_model.get_topic_info()

            safe_cat_name = str(cat).replace("/", "_").replace(" ", "_")
            save_path = os.path.join(LABELED_FOLDER, f"topic_summary_{safe_cat_name}.xlsx")
            topic_info.to_excel(save_path, index=False)

            print(f"âœ… [{cat}] ì €ì¥ ì™„ë£Œ â†’ {save_path}")

        except Exception as e:
            print(f"âŒ [{cat}] ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")


def main():
    excel_files = glob.glob(os.path.join(LABELED_FOLDER, "*.xlsx"))

    if not excel_files:
        print(f"[ì˜¤ë¥˜] {LABELED_FOLDER} í´ë”ì— ì—‘ì…€ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    for file in excel_files:
        topic_modeling_per_category_in_file(file)

    print("\nğŸ¯ ëª¨ë“  categoryë³„ í† í”½ëª¨ë¸ë§ ì™„ë£Œ!")


if __name__ == "__main__":
    main()
