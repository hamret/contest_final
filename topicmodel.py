import os
import pandas as pd
from tqdm import tqdm
from bertopic import BERTopic
from sentence_transformers import SentenceTransformer
from umap import UMAP
from sklearn.feature_extraction.text import CountVectorizer

# ê²½ë¡œ ì„¤ì •
DATA_DIR = r"C:\Users\user\PycharmProjects\PythonProject3\work"  # ë³¸ì¸ ë°ì´í„°ì…‹ í´ë”ë¡œ ë³€ê²½
SAVE_DIR = os.path.join(DATA_DIR, "topic_results")
os.makedirs(SAVE_DIR, exist_ok=True)

# í•œêµ­ì–´ ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ (ê¸°ì¡´ì— ìˆë˜ ë¦¬ìŠ¤íŠ¸ í™œìš©í•˜ê±°ë‚˜ í•„ìš”ì‹œ ìˆ˜ì •)
stopwords = [
    'ê¸°ì','ë³´ë„','ê´€ë ¨','ê´€ê³„ì','ìœ„í•´','í†µí•´','ì´ë²ˆ','ê²ƒ','ë“±',
    'ì œì •','ì‹œí–‰','ë°œí‘œ','ë°í˜”ë‹¤','ì „í–ˆë‹¤','ë…¼ì˜','ìœ„í•œ',
    'ìœ¤ì„ì—´','ì´ì¬ëª…','ê¹€ê±´í¬','êµ­ë¯¼ì˜í˜','ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹','ì •ë‹¹','ëŒ€í†µë ¹','ì´ë¦¬'
]

# UMAP ëª¨ë¸ ì„¤ì •
umap_model = UMAP(n_neighbors=15, n_components=5, metric='cosine', random_state=42)

# ë¬¸ì¥ ì„ë² ë”© ëª¨ë¸ (KoSentCSE ë¡œë²„íƒ€ ë©€í‹°íƒœìŠ¤í¬)
embedding_model = SentenceTransformer("BM-K/KoSimCSE-roberta-multitask")

# í•´ë‹¹ í´ë” ë‚´ ëª¨ë“  Excel íŒŒì¼ ì²˜ë¦¬
excel_files = [os.path.join(DATA_DIR, f) for f in os.listdir(DATA_DIR) if f.endswith(".xlsx")]

for file_path in excel_files:
    print(f"\nğŸ“‚ íŒŒì¼ ì²˜ë¦¬: {os.path.basename(file_path)}")
    df = pd.read_excel(file_path)

    # í™•ì¸í•  ì»¬ëŸ¼ëª…, í•„ìš”ì‹œ ìˆ˜ì •
    if "category" not in df.columns or "content" not in df.columns:
        print(f"âš ï¸ category ë˜ëŠ” content ì»¬ëŸ¼ ëˆ„ë½: {file_path}")
        continue

    categories = df["category"].dropna().unique()

    for cat in categories:
        cat_df = df[df["category"] == cat]
        texts = cat_df["content"].dropna().tolist()

        # ë¬¸ë‹¨ ë‹¨ìœ„ ë¶„í• , 30ì ì´ìƒë§Œ
        split_texts = []
        for text in texts:
            paragraphs = [p.strip() for p in text.split("\n") if len(p.strip()) > 30]
            split_texts.extend(paragraphs)

        if len(split_texts) < 10:
            print(f"âš ï¸ {cat} ë°ì´í„°ê°€ ë„ˆë¬´ ì ì–´ ìŠ¤í‚µ ({len(split_texts)}ê°œ)")
            continue

        print(f"\nğŸš€ {cat} í† í”½ ëª¨ë¸ë§ ì‹œì‘ (ë¬¸ë‹¨ ìˆ˜: {len(split_texts)})")

        # min_df ìë™ ì¡°ì •
        if len(split_texts) < 500:
            min_df_value = 1
        elif len(split_texts) < 2000:
            min_df_value = 2
        else:
            min_df_value = 3

        vectorizer_model = CountVectorizer(
            stop_words=stopwords,
            ngram_range=(1, 2),
            min_df=min_df_value
        )

        # BERTopic ëª¨ë¸ ì´ˆê¸°í™”
        topic_model = BERTopic(
            embedding_model=embedding_model,
            umap_model=umap_model,
            vectorizer_model=vectorizer_model,
            top_n_words=10,
            calculate_probabilities=False,
            verbose=False
        )

        try:
            topics, probs = topic_model.fit_transform(split_texts)
            topic_model.update_topics(split_texts, topics, vectorizer_model=vectorizer_model)

            topic_info = topic_model.get_topic_info()
            save_path = os.path.join(SAVE_DIR, f"topic_summary_{cat}.xlsx")
            topic_info.to_excel(save_path, index=False)
            print(f"âœ… ì €ì¥ ì™„ë£Œ: {save_path}")

        except Exception as e:
            print(f"âŒ {cat} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

print("\nğŸ‰ ëª¨ë“  íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ!")
